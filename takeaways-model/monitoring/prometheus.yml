global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  - job_name: 'takeaways-api'
    static_configs:
      - targets: ['takeaways-api:8000']
    metrics_path: '/metrics'
    scheme: http

  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

rules:
  - name: model_metrics
    rules:
      # Response time alerts
      - alert: HighResponseTime
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: High response time (instance {{ $labels.instance }})
          description: "95th percentile response time is above 1s"

      # Error rate alerts
      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m]) > 0.1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: High error rate (instance {{ $labels.instance }})
          description: "Error rate is above 10%"

      # GPU memory usage
      - alert: HighGPUMemoryUsage
        expr: nvidia_gpu_memory_used_bytes / nvidia_gpu_memory_total_bytes > 0.9
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: High GPU memory usage (instance {{ $labels.instance }})
          description: "GPU memory usage is above 90%"

      # Model latency
      - alert: ModelLatencyHigh
        expr: rate(model_inference_duration_seconds_sum[5m]) / rate(model_inference_duration_seconds_count[5m]) > 2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: High model inference latency (instance {{ $labels.instance }})
          description: "Average model inference time is above 2 seconds"
